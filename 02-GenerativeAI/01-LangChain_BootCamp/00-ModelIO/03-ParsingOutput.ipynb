{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Output\n",
    "\n",
    "Often when connecting to outputs of LLMs we need the output in specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.output_parsers import  CommaSeparatedListOutputParser, DatetimeOutputParser, PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two', 'three']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to create a proper prompt using this\n",
    "reply = \"one, two, three\"\n",
    "output_parser.parse(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='give me 5 characteristics of pet dogs. Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_template = '{request}{format_instructions}'\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "\n",
    "chat_prompt.format_prompt(request=\"give me 5 characteristics of pet dogs. \", format_instructions=output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/r3wsv_bd0bs1_vcy4pb5q9pr0000gn/T/ipykernel_83956/3579546247.py:4: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model(request)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loyal, playful, social, protective, affectionate\n"
     ]
    }
   ],
   "source": [
    "# Pass to the model\n",
    "request = chat_prompt.format_prompt(request=\"give me 5 characteristics of pet dogs. \", format_instructions=output_parser.get_format_instructions()).to_messages()\n",
    "\n",
    "response = model(request)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loyal', 'playful', 'social', 'protective', 'affectionate']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can convert to desired output - output in the form of a list\n",
    "output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0422-02-11T16:24:19.265732Z, 1887-08-16T21:49:20.340637Z, 1964-10-13T16:26:34.139412Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "output_parser = DatetimeOutputParser()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(template_text)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: When was the 13th Ammendment ratified in the US?\n",
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0544-04-24T03:32:17.788149Z, 0533-03-09T09:58:27.144409Z, 0037-12-31T19:48:45.559557Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "# Construct the prompt and pass to the model\n",
    "print(chat_prompt.format(request=\"When was the 13th Ammendment ratified in the US?\", format_instructions=output_parser.get_format_instructions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1865-12-06T00:00:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "request = chat_prompt.format_prompt(request=\"When was the 13th Ammendment ratified in the US?\", format_instructions=output_parser.get_format_instructions()).to_messages()\n",
    "response = model(request, temperature = 0)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1865, 12, 6, 0, 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic JSON Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE JSON STRUCTURE\n",
    "class Scientist(BaseModel):\n",
    "\n",
    "    name: str = Field(description=\"Name of a Scientist\")\n",
    "    discoveries: list = Field(description=\"Python list of discoveries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Name any famous Scientist and list of his/her discoveries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of a Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Scientist)\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of a Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\n",
      "```\n",
      "Tell me about any famous Scientist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct the Prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseModel.json of Scientist(name='Albert Einstein', discoveries=['Theory of Relativity', 'Photoelectric Effect', 'Brownian Motion', 'Mass-Energy Equivalence (E=mc^2)'])>\n"
     ]
    }
   ],
   "source": [
    "# Pass to the model\n",
    "\n",
    "#my_query=\"Tell me about any famous Scientist\"\n",
    "chain = prompt | model | parser\n",
    "response = chain.invoke({\"query\": \"Tell me about any famous Scientist\"})\n",
    "\n",
    "print(response.json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
