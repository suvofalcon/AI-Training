{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large Language Models\n",
    "\n",
    "## Working with Text Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that LangChain automatically looks up for any environment variable with the name OPENAI_API_KEY automatically when making a connection to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Text Model\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(openai_api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Pluto was discovered in 1930 by American astronomer Clyde Tombaugh, who was only 24 years old at the time. He spent nearly a year comparing photographs of the night sky, looking for a ninth planet predicted by astronomer Percival Lowell. Tombaugh ultimately found Pluto by noticing a moving object in the photographs that did not match the expected background stars.\n"
     ]
    }
   ],
   "source": [
    "# Lets work on a simplest way to get a text autocomplete -\n",
    "print(llm.invoke(\"Here is a fun fact about Pluto Planet :\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'BaseMessage': {'additionalProperties': True,\n",
       "   'description': 'Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'type'],\n",
       "   'title': 'BaseMessage',\n",
       "   'type': 'object'},\n",
       "  'BaseMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk, which can be concatenated with other Message chunks.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'type'],\n",
       "   'title': 'BaseMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatGeneration': {'description': 'A single chat generation output.\\n\\nA subclass of Generation that represents the response from a chat model\\nthat generates chat messages.\\n\\nThe `message` attribute is a structured representation of the chat message.\\nMost of the time, the message will be of type `AIMessage`.\\n\\nUsers working with chat models will usually access information via either\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks).',\n",
       "   'properties': {'text': {'default': '', 'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'ChatGeneration',\n",
       "     'default': 'ChatGeneration',\n",
       "     'enum': ['ChatGeneration'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'message': {'$ref': '#/$defs/BaseMessage'}},\n",
       "   'required': ['message'],\n",
       "   'title': 'ChatGeneration',\n",
       "   'type': 'object'},\n",
       "  'ChatGenerationChunk': {'description': 'ChatGeneration chunk, which can be concatenated with other\\nChatGeneration chunks.',\n",
       "   'properties': {'text': {'default': '', 'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'ChatGenerationChunk',\n",
       "     'default': 'ChatGenerationChunk',\n",
       "     'enum': ['ChatGenerationChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'message': {'$ref': '#/$defs/BaseMessageChunk'}},\n",
       "   'required': ['message'],\n",
       "   'title': 'ChatGenerationChunk',\n",
       "   'type': 'object'},\n",
       "  'Generation': {'description': 'A single text generation output.\\n\\nGeneration represents the response from an \"old-fashioned\" LLM that\\ngenerates regular text (not chat messages).\\n\\nThis model is used internally by chat model and will eventually\\nbe mapped to a more general `LLMResult` object, and then projected into\\nan `AIMessage` object.\\n\\nLangChain users working with chat models will usually access information via\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation\\nfor more information.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'Generation',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'Generation',\n",
       "   'type': 'object'},\n",
       "  'GenerationChunk': {'description': 'Generation chunk, which can be concatenated with other Generation chunks.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'Generation',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'GenerationChunk',\n",
       "   'type': 'object'},\n",
       "  'RunInfo': {'description': 'Class that contains metadata for a single execution of a Chain or model.\\n\\nDefined for backwards compatibility with older versions of langchain_core.\\n\\nThis model will likely be deprecated in the future.\\n\\nUsers can acquire the run_id information from callbacks or via run_id\\ninformation present in the astream_event API (depending on the use case).',\n",
       "   'properties': {'run_id': {'format': 'uuid',\n",
       "     'title': 'Run Id',\n",
       "     'type': 'string'}},\n",
       "   'required': ['run_id'],\n",
       "   'title': 'RunInfo',\n",
       "   'type': 'object'}},\n",
       " 'description': 'A container for results of an LLM call.\\n\\nBoth chat models and LLMs generate an LLMResult object. This object contains\\nthe generated outputs and any additional information that the model provider\\nwants to return.',\n",
       " 'properties': {'generations': {'items': {'items': {'anyOf': [{'$ref': '#/$defs/Generation'},\n",
       "      {'$ref': '#/$defs/ChatGeneration'},\n",
       "      {'$ref': '#/$defs/GenerationChunk'},\n",
       "      {'$ref': '#/$defs/ChatGenerationChunk'}]},\n",
       "    'type': 'array'},\n",
       "   'title': 'Generations',\n",
       "   'type': 'array'},\n",
       "  'llm_output': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Llm Output'},\n",
       "  'run': {'anyOf': [{'items': {'$ref': '#/$defs/RunInfo'}, 'type': 'array'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Run'},\n",
       "  'type': {'const': 'LLMResult',\n",
       "   'default': 'LLMResult',\n",
       "   'enum': ['LLMResult'],\n",
       "   'title': 'Type',\n",
       "   'type': 'string'}},\n",
       " 'required': ['generations'],\n",
       " 'title': 'LLMResult',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets generate a full output with mmore information\n",
    "result = llm.generate([\"Here is Fun Fact about Pluto : \",\n",
    "                       \"Here is a Fun Fact about Mars :\"])\n",
    "\n",
    "# to check the schema of the response\n",
    "result.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nPluto's largest moon, Charon, is almost half the size of Pluto itself. This makes it the largest moon relative to its planet in our solar system.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the detailed response\n",
    "response = result.generations\n",
    "response[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 112,\n",
       "  'prompt_tokens': 16,\n",
       "  'completion_tokens': 96},\n",
       " 'model_name': 'gpt-3.5-turbo-instruct'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the token usage and other details about output of the LLM\n",
    "\n",
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! One fact about Earth is that it is the only planet in our solar system known to support life.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 16, 'total_tokens': 38, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3d257724-fdbc-4f09-9885-99dd9a14af86-0', usage_metadata={'input_tokens': 16, 'output_tokens': 22, 'total_tokens': 38})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets ask a question\n",
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Earth ?\")])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! One fact about Earth is that it is the only planet in our solar system known to support life.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the text response\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! One interesting fact about Earth is that it is the only known planet in our solar system that has liquid water on its surface, which is essential for supporting life as we know it.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try another\n",
    "result = chat.generate([[SystemMessage(content=\"You are a University Professor\"),\n",
    "                         HumanMessage(content=\"Can you tell me a fact about Earth ?\")]])\n",
    "\n",
    "result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature is a parameter that controls the “creativity” or randomness of the text generated by open ai llm. A higher temperature (e.g., 0.7) results in more diverse and creative output, while a lower temperature (e.g., 0.2) makes the output more deterministic and focused.\n",
    "\n",
    "- In practice, temperature affects the probability distribution over the possible tokens at each step of the generation process. A temperature of 0 would make the model completely deterministic, always choosing the most likely token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here is a fact about Earth :\\n\\nApproximately 71% of Earth's surface is covered in water, making it unique among the planets in our solar system.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can add some extra parameters and args - here we are adding some extreme values\n",
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Earth ?\")],\n",
    "                     temperature=2, max_tokens=100)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "Should only do this if the prompt is the exact same and the historical replies are okay to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Initialize the cache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Mars is the fourth planet from the sun in our solar system, located between Earth and Jupiter.\\n\\n2. The planet is named after the Roman god of war, likely due to its reddish appearance, which is caused by iron oxide (rust) on its surface.\\n\\n3. Mars has a thin atmosphere composed mainly of carbon dioxide, with traces of nitrogen and argon.\\n\\n4. The planet has two small moons, Phobos and Deimos, which are thought to be captured asteroids.\\n\\n5. Mars is often called the \"Red Planet\" because of its rusty color, but it also has white polar ice caps made of frozen carbon dioxide and water.\\n\\n6. The largest volcano in the solar system, Olympus Mons, is located on Mars and stands at a height of 22 km (14 miles).\\n\\n7. Mars has a day and night cycle similar to Earth, with a day lasting approximately 24 hours and 37 minutes.\\n\\n8. The average temperature on Mars is around -80°F (-62°C), but it can reach highs of 68°F (20°C) at the equator during summer.\\n\\n9. Evidence suggests that Mars once had a much thicker atmosphere and flowing water on its surface, making it a potential candidate for past or present extrater'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first time it is not in cache and hence might take little time\n",
    "llm.invoke(\"Tell me fact about Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Mars is the fourth planet from the sun in our solar system, located between Earth and Jupiter.\\n\\n2. The planet is named after the Roman god of war, likely due to its reddish appearance, which is caused by iron oxide (rust) on its surface.\\n\\n3. Mars has a thin atmosphere composed mainly of carbon dioxide, with traces of nitrogen and argon.\\n\\n4. The planet has two small moons, Phobos and Deimos, which are thought to be captured asteroids.\\n\\n5. Mars is often called the \"Red Planet\" because of its rusty color, but it also has white polar ice caps made of frozen carbon dioxide and water.\\n\\n6. The largest volcano in the solar system, Olympus Mons, is located on Mars and stands at a height of 22 km (14 miles).\\n\\n7. Mars has a day and night cycle similar to Earth, with a day lasting approximately 24 hours and 37 minutes.\\n\\n8. The average temperature on Mars is around -80°F (-62°C), but it can reach highs of 68°F (20°C) at the equator during summer.\\n\\n9. Evidence suggests that Mars once had a much thicker atmosphere and flowing water on its surface, making it a potential candidate for past or present extrater'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This response should come very fast\n",
    "llm.invoke(\"Tell me fact about Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
