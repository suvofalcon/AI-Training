{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Transformers and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('some_data/FDR_State_of_Union_1944.txt') as file:\n",
    "    speech_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters - 21927\n",
      "Number of Words - 3750\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Characters - {len(speech_text)}\")\n",
    "print(f\"Number of Words - {len(speech_text.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "\n",
      "page_content='This Nation in the past two years has become an active partner in the world's greatest war against human slavery.\n",
      "\n",
      "We have joined with like-minded people in order to defend ourselves in a world that has been gravely threatened with gangster rule.\n",
      "\n",
      "But I do not think that any of us Americans can be content with mere survival. Sacrifices that we and our allies are making impose upon us all a sacred obligation to see to it that out of this war we and our children will gain something better than mere survival.\n",
      "\n",
      "We are united in determination that this war shall not be followed by another interim which leads to new disaster- that we shall not repeat the tragic errors of ostrich isolationism—that we shall not repeat the excesses of the wild twenties when this Nation went for a joy ride on a roller coaster which ended in a tragic crash.'\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=1000)\n",
    "texts = text_splitter.create_documents([speech_text])\n",
    "print(type(texts))\n",
    "print(\"\\n\")\n",
    "print(texts[0]) # display the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Chunks - 28\n",
      "Length of the first chunk - 841\n",
      "\n",
      "\n",
      "This Nation in the past two years has become an active partner in the world's greatest war against human slavery.\n",
      "\n",
      "We have joined with like-minded people in order to defend ourselves in a world that has been gravely threatened with gangster rule.\n",
      "\n",
      "But I do not think that any of us Americans can be content with mere survival. Sacrifices that we and our allies are making impose upon us all a sacred obligation to see to it that out of this war we and our children will gain something better than mere survival.\n",
      "\n",
      "We are united in determination that this war shall not be followed by another interim which leads to new disaster- that we shall not repeat the tragic errors of ostrich isolationism—that we shall not repeat the excesses of the wild twenties when this Nation went for a joy ride on a roller coaster which ended in a tragic crash.\n",
      "\n",
      "\n",
      "Type of each of the chunk - <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of Chunks - {len(texts)}\")\n",
    "print(f\"Length of the first chunk - {len(texts[0].page_content)}\")\n",
    "print(\"\\n\")\n",
    "print(texts[0].page_content)\n",
    "print(\"\\n\")\n",
    "print(f\"Type of each of the chunk - {type(texts[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Chunks - 15\n",
      "Length of the first chunk - 2332\n",
      "\n",
      "\n",
      "This Nation in the past two years has become an active partner in the world's greatest war against human slavery.\n",
      "\n",
      "We have joined with like-minded people in order to defend ourselves in a world that has been gravely threatened with gangster rule.\n",
      "\n",
      "But I do not think that any of us Americans can be content with mere survival. Sacrifices that we and our allies are making impose upon us all a sacred obligation to see to it that out of this war we and our children will gain something better than mere survival.\n",
      "\n",
      "We are united in determination that this war shall not be followed by another interim which leads to new disaster- that we shall not repeat the tragic errors of ostrich isolationism—that we shall not repeat the excesses of the wild twenties when this Nation went for a joy ride on a roller coaster which ended in a tragic crash.\n",
      "\n",
      "When Mr. Hull went to Moscow in October, and when I went to Cairo and Teheran in November, we knew that we were in agreement with our allies in our common determination to fight and win this war. But there were many vital questions concerning the future peace, and they were discussed in an atmosphere of complete candor and harmony.\n",
      "\n",
      "In the last war such discussions, such meetings, did not even begin until the shooting had stopped and the delegates began to assemble at the peace table. There had been no previous opportunities for man-to-man discussions which lead to meetings of minds. The result was a peace which was not a peace. That was a mistake which we are not repeating in this war.\n",
      "\n",
      "And right here I want to address a word or two to some suspicious souls who are fearful that Mr. Hull or I have made \"commitments\" for the future which might pledge this Nation to secret treaties, or to enacting the role of Santa Claus.\n",
      "\n",
      "To such suspicious souls—using a polite terminology—I wish to say that Mr. Churchill, and Marshal Stalin, and Generalissimo Chiang Kai-shek are all thoroughly conversant with the provisions of our Constitution. And so is Mr. Hull. And so am I.\n",
      "\n",
      "Of course we made some commitments. We most certainly committed ourselves to very large and very specific military plans which require the use of all Allied forces to bring about the defeat of our enemies at the earliest possible time.\n",
      "\n",
      "But there were no secret treaties or political or financial commitments.\n",
      "\n",
      "\n",
      "Type of each of the chunk - <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500) # Now chunk size is hard length based on tokens\n",
    "texts = text_splitter.split_text(speech_text)\n",
    "\n",
    "print(f\"Total Number of Chunks - {len(texts)}\")\n",
    "print(f\"Length of the first chunk - {len(texts[0])}\")\n",
    "print(\"\\n\")\n",
    "print(texts[0])\n",
    "print(\"\\n\")\n",
    "print(f\"Type of each of the chunk - {type(texts[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between using CharacterTextSplitter.from_tiktoken_encoder() and CharacterTextSplitter in LangChain lies in how they determine chunk size:\n",
    "\n",
    "CharacterTextSplitter:\n",
    "\n",
    "Splits text into chunks based on the number of characters.\n",
    "chunk_size parameter directly controls the number of characters in each chunk.\n",
    "Simple and straightforward, but might not be the most efficient for language models.\n",
    "CharacterTextSplitter.from_tiktoken_encoder():\n",
    "\n",
    "Splits text into chunks based on the number of tokens, as determined by the specified tiktoken encoder.\n",
    "chunk_size parameter now refers to the maximum number of tokens allowed in a chunk.\n",
    "More accurate for language models as they operate on tokens, not characters. This ensures chunks are more consistent with model limitations.\n",
    "In essence:\n",
    "\n",
    "Use CharacterTextSplitter if you want to split text by a fixed number of characters.\n",
    "Use CharacterTextSplitter.from_tiktoken_encoder() if you want to split text by a fixed number of tokens, which is generally more suitable for optimal performance with language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "text = \"Some normal text to send to OpenAI to be embedded into a N dimensional vector\"\n",
    "embedded_text = embeddings.embed_query(text=text)\n",
    "type(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the embedded text - 1536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the embedded text - {len(embedded_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader('some_data/penguins.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data - <class 'list'>\n",
      "Type of every element in the data - <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of data - {type(data)}\")\n",
    "print(f\"Type of every element in the data - {type(data[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we embed each of the document\n",
    "embedded_docs = embeddings.embed_documents([text.page_content for text in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the embeded_docs - 344\n",
      "Length of each of the item in the embedded docs - 1536\n",
      "Type of the embedded_docs - <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the embeded_docs - {len(embedded_docs)}\")\n",
    "print(f\"Length of each of the item in the embedded docs - {len(embedded_docs[0])}\")\n",
    "print(f\"Type of the embedded_docs - {type(embedded_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
